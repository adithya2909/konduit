# ğŸ§  RAG Web â€“ Website Crawler + Retrieval-Augmented Generation Pipeline

A modular **Retrieval-Augmented Generation (RAG)** system that crawls websites, extracts and indexes their textual content, embeds it into a vector store, retrieves the most relevant chunks, and generates **grounded, source-cited answers** using an LLM.

---

## âš™ï¸ Setup

1. **Clone and enter the project**
   git clone https://github.com/jaipratham/Website-Crawler.git
   cd Website-Crawler
   
2.Create and activate a virtual environment
  python -m venv venv
  source venv/bin/activate        # On Windows: venv\Scripts\activate

3.Install dependencies
  pip install -r requirements.txt

4.Configure settings
  Edit config/settings.yaml to set crawl depth, embedding model, chunk size, vector store type, etc.

# ğŸ•¸ï¸ Website-Crawler RAG Pipeline

This project implements a comprehensive Retrieval-Augmented Generation (RAG) pipeline that uses a website crawler to build its knowledge base.

## ğŸš€ Architecture Overview
```

Website-Crawler/
â”œâ”€â”€ main.py                 # Entry point for the RAG pipeline
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.yaml       # Configuration (crawl depth, model, embedding type, etc.)
â”‚
â”œâ”€â”€ crawler/
â”‚   â”œâ”€â”€ crawler.py          # Crawls website (respects robots.txt, domain limits)
â”‚   â””â”€â”€ parser.py           # Cleans & extracts text from HTML pages
â”‚
â”œâ”€â”€ indexing/
â”‚   â”œâ”€â”€ chunker.py          # Splits text into chunks
â”‚   â”œâ”€â”€ embedder.py         # Embeds chunks via OpenAI / SentenceTransformers
â”‚   â””â”€â”€ vectorstore.py      # Stores and retrieves embeddings (FAISS / Chroma)
â”‚
â”œâ”€â”€ retrieval/
â”‚   â””â”€â”€ retriever.py        # Retrieves top-k relevant chunks
â”‚
â”œâ”€â”€ generation/
â”‚   â””â”€â”€ generator.py        # Generates grounded answers using retrieved context
â”‚
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ evaluate.py         # Computes recall@k, grounding correctness, etc.
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logger.py           # Logging, timing, and error handling utilities
â”‚
â””â”€â”€ data/
    â”œâ”€â”€ raw_html/           # Cached raw HTML pages
    â”œâ”€â”€ cleaned_text/       # Cleaned text extracted from pages
    â””â”€â”€ index/              # Stored embeddings / vector DB '''

```

ğŸ“Š Config (config/settings.yaml)
```
crawl:
  max_depth: 2
  delay: 500
  max_pages: 20

embedding:
  model: sentence-transformers/all-MiniLM-L6-v2 # opens source model from Huggingface for embeddings.
  chunk_size: 256
  overlap: 50

vectorstore:
  backend: faiss
  dimension: 384

generation:
  model: llama-3.1-8b-instant
  max_tokens: 500
  temperature: 0.0
  ```

âš–ï¸ Design Trade-offs:

âœ… Modular architecture â€“ Each stage (crawl â†’ index â†’ retrieve â†’ generate) is isolated and reusable.

âœ… Config-driven â€“ Change depth, embedding model, and vector DB via YAML.

âœ… Embeddings backend â€“ Supports FAISS or Chroma seamlessly.

âœ… LLM flexibility â€“ Works with OpenAI, HuggingFace, or local models.

âœ… Respects robots.txt â€“ Ethical crawling with domain limits and delay.

âœ… Logging included â€“ Detailed logs and runtime metrics in utils/logger.py.

âœ… Lightweight dependencies â€“ Pure Python, no JS rendering or heavy frameworks.

âš ï¸ No dynamic JS rendering â€“ Wonâ€™t extract content from SPA-heavy pages.

âš ï¸ Basic evaluation metrics â€“ Extend for F1, ROUGE, BLEU, etc.

âš ï¸ Local storage â€“ Data and vector DB stored in /data/, migrate for scale.

âœ… Easy to extend â€“ Add retrieval strategies or chain-of-thought generators.

âœ… Compatible with LangChain / LlamaIndex â€“ Plug-and-play design.

âœ… Portable â€“ Works locally or via Docker.

âœ… End-to-end reproducible â€“ Crawl, embed, retrieve, and generate in one run.

ğŸ§‘â€ğŸ’» Author

Developed by Jaipratham
For learning, research, and practical experimentation with RAG systems.
